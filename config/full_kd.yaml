seed: 0
base_model: deepseek-ai/DeepSeek-R1-Distill-Qwen-7B
run_name: "INPUT"
student_model_path: ./models/${base_model}/compressed/${run_name}/model
verbose: True
log_with: wandb
# ctx_len: 8192

#dataset level config
dataset_config_base_path: config/dataset/
datasets:
  - ctx_len: 128
    dataset_config: SlimPajama-627B
    n_samples: 128


# Training configuration
training:
  output_dir: ./models/${base_model}/ft/${run_name}
  epochs: 10
  batch_size: 1
  train_val_split: 0.9
  gradient_accumulation_steps: 4
  mixed_precision: bf16
  optimizer:
    _target_: torch.optim.AdamW
    lr: 1.0e-6
    weight_decay: 0.00
  max_grad_norm: 1000000000000000.0
  scheduler: CONSTANT
  # scheduler:
  #   _target_: transformers.get_linear_schedule_with_warmup
  #   num_warmup_steps: 10
    # num_training_steps is calculated in the script

# Knowledge Distillation configuration
distillation:
  temperature: 2.0
  alpha: 0.0 # Weight for activation loss. Set to 0 to disable.
  use_activation_loss: False
  freeze_embedding: True # Freeze the embedding layer of the student model
  freeze_lm_head: False # Freeze the output layer of the student model