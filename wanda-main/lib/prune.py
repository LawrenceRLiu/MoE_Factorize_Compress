import time 
import heapq 
import torch 
import torch.nn as nn 
import gc
import copy
import tqdm
from transformers import AutoModelForCausalLM, AutoTokenizer
from typing import Any, Callable, Dict, List, Optional, Tuple
from .sparsegpt import SparseGPT 
from .layerwrapper import WrappedGPT
from .data import get_loaders 

from .ablate import AblateGPT 


### NEW CODE COPIED FROM MY IMPLEMENTATION TO WORK WITH THE NEWER TRANSFORMERS VERSION

def clean():
    gc.collect()
    torch.cuda.empty_cache()
    torch.cuda.ipc_collect()

class ForwardStopError(Exception):
    """Custom exception to stop the forward pass in a model."""
    pass

def get_inps(model: AutoModelForCausalLM, 
             tokens: torch.Tensor, 
             attention_mask: Optional[torch.Tensor] = None,
             offload_activations: bool = False,
             dev: Optional[torch.device] = None
             )->Tuple[torch.Tensor, Tuple[Any],Dict[str, Any]]:
    
    """Generate inputs for the model by running a forward pass.

    Args:
        model (AutoModelForCausalLM): The model to use for inference.
        tokens (torch.Tensor): Tokens to pass into the model of shape (n_samples, seqlen).
        attention_mask (Optional[torch.Tensor], optional): Custom attention mask of
            shape (n_samples, seqlen, seqlen) or None if we just use the default mask.
            Defaults to None.
    Raises:
        CatcherException: _description_

    Returns:
        torch.Tensor: Inputs of shape (n_samples, seqlen, hidden_size) generated by the model.
        Tuple[Any]: A tuple containing the captured arguments.
        Dict[str, Any]: A dictionary containing the captured keyword arguments.
    """
    
    print("Starting...")
    if dev is None:
        dev = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    n_samples, seqlen = tokens.shape
    use_cache = model.config.use_cache
    model.config.use_cache = False
    layers = model.model.layers

    model.model.embed_tokens = model.model.embed_tokens.to(dev)
    model.model.norm = model.model.norm.to(dev)
    model.model.rotary_emb = model.model.rotary_emb.to(dev)
    layers[0] = layers[0].to(dev)

    dtype = next(iter(model.parameters())).dtype
    inps = torch.zeros(
        (n_samples, seqlen, model.config.hidden_size),
        dtype=dtype,
        device=dev if not offload_activations else "cpu",
    )
    # ========== Preprocessing the data ==========
    train_cache = {"i": 0, "kwargs": {},
                   "args": ()}
    
    class Catcher(torch.nn.Module):
        def __init__(self, original_module):
            super().__init__()
            # Use object.__setattr__ to avoid triggering our custom __setattr__
            object.__setattr__(self, '_wrapped_module', original_module)

        def forward(self,  inp, *args, **kwargs):
            inps[train_cache["i"]] = inp if not offload_activations else inp.cpu()
            train_cache["i"] += 1
            train_cache["kwargs"] = kwargs
            train_cache["args"] = args
            raise ForwardStopError("Forward pass interrupted by Catcher")

        def __getattr__(self, name):
            # Forward attribute gets to the wrapped module if not found on self
            return getattr(self._wrapped_module, name)

        def __setattr__(self, name, value):
            # Forward attribute sets to the wrapped module unless it's our private attrs
            private_attrs = {'_wrapped_module', 'captured_args_kwargs'}
            if name in private_attrs:
                object.__setattr__(self, name, value)
            else:
                setattr(self._wrapped_module, name, value)
    
    layer_clone = copy.deepcopy(layers[0])
    layers[0] = Catcher(layers[0])
    
    #catch the inputs
    with torch.no_grad():
        for i, sequence in enumerate(tqdm.tqdm(
            tokens, desc="getting inputs", miniters=len(tokens) // 100
        )):
            try:
                model(sequence.to(dev).unsqueeze(0), attention_mask=attention_mask[i].unsqueeze(0) if attention_mask is not None else None)
            except ForwardStopError:
                pass
    layers[0] = layer_clone
    
    layers[0] = layers[0].cpu()
    model.model.embed_tokens = model.model.embed_tokens.cpu()
    model.model.norm = model.model.norm.cpu()
    model.model.rotary_emb = model.model.rotary_emb.cpu()
    
    clean()
    
    #pass the kwargs and args to device
    args = train_cache["args"]
    kwargs = train_cache["kwargs"]
    # if the attention mask is not None, then we need to remove the attention mask from the kwargs
    if attention_mask is not None:
        if "attention_mask" in kwargs:
            del kwargs["attention_mask"]
    for i, arg in enumerate(args):
        if isinstance(arg, torch.Tensor):
            args[i] = arg.to(dev)
    for key, value in kwargs.items():
        if isinstance(value, torch.Tensor):
            kwargs[key] = value.to(dev)
            
    return inps, args, kwargs


def find_layers(module, layers=[nn.Linear], name=''):
    """
    Recursively find the layers of a certain type in a module.

    Args:
        module (nn.Module): PyTorch module.
        layers (list): List of layer types to find.
        name (str): Name of the module.

    Returns:
        dict: Dictionary of layers of the given type(s) within the module.
    """
    if type(module) in layers:
        return {name: module}
    res = {}
    for name1, child in module.named_children():
        res.update(find_layers(
            child, layers=layers, name=name + '.' + name1 if name != '' else name1
        ))
    return res

def check_sparsity(model):
    use_cache = model.config.use_cache 
    model.config.use_cache = False 

    layers = model.model.layers
    count = 0 
    total_params = 0
    for i in range(len(layers)):
        layer = layers[i]
        subset = find_layers(layer)

        sub_count = 0
        sub_params = 0
        for name in subset:
            W = subset[name].weight.data
            count += (W==0).sum().item()
            total_params += W.numel()

            sub_count += (W==0).sum().item()
            sub_params += W.numel()

        print(f"layer {i} sparsity {float(sub_count)/sub_params:.6f}")

    model.config.use_cache = use_cache 
    return float(count)/total_params 


def return_given_alpha(alpha, sort_res, W_metric, tmp_metric, sum_before):
    thres_cumsum = sum_before * alpha 
    sort_mask = tmp_metric <= thres_cumsum.reshape((-1,1))
    thres = torch.gather(sort_res[0], dim=1, index=sort_mask.sum(dim=1, keepdims=True)-1)
    W_mask = (W_metric <= thres)
    cur_sparsity = (W_mask==True).sum() / W_mask.numel()
    return W_mask, cur_sparsity

def prune_magnitude(args, model, tokenizer, device=torch.device("cuda:0"), prune_n=0, prune_m=0):
    layers = model.model.layers 

    for i in range(len(layers)):
        layer = layers[i]
        subset = find_layers(layer)

        for name in subset:
            W = subset[name].weight.data 
            W_metric = torch.abs(W)
            if prune_n != 0:
                W_mask = (torch.zeros_like(W)==1)
                for ii in range(W_metric.shape[1]):
                    if ii % prune_m == 0:
                        tmp = W_metric[:,ii:(ii+prune_m)].float()
                        W_mask.scatter_(1,ii+torch.topk(tmp, prune_n,dim=1, largest=False)[1], True)
            else:
                thresh = torch.sort(W_metric.flatten().cuda())[0][int(W.numel()*args.sparsity_ratio)].cpu()
                W_mask = (W_metric<=thresh)

            W[W_mask] = 0

def prune_wanda(args, model, tokenizer, device=torch.device("cuda:0"), prune_n=0, prune_m=0):
    use_cache = model.config.use_cache 
    model.config.use_cache = False 

    print("loading calibdation data")
    dataloader, _ = get_loaders("c4",nsamples=args.nsamples,seed=args.seed,seqlen=model.seqlen,tokenizer=tokenizer)
    #adapt the dataloader to the format we are expecting
    tokens = torch.stack([d[0][0] for d in dataloader])
    print("tokens shape", tokens.shape)
    print("dataset loading complete")
    with torch.no_grad():
        inps, model_args, model_kwargs = get_inps(model, tokens, offload_activations=True, dev=device)
        outs = torch.zeros_like(inps)
    #cast the entire model to cpu
    # model.to("cpu")
    layers = model.model.layers
    for i in range(len(layers)):
        layer = layers[i]
        #cast layer to device
        layer.to(device)
        
        
        subset = find_layers(layer)


        wrapped_layers = {}
        for name in subset:
            wrapped_layers[name] = WrappedGPT(subset[name])

        def add_batch(name):
            def tmp(_, inp, out):
                wrapped_layers[name].add_batch(inp[0].data, out.data)
            return tmp

        handles = []
        for name in wrapped_layers:
            handles.append(subset[name].register_forward_hook(add_batch(name)))
        for j in range(args.nsamples):
            with torch.no_grad():
                outs[j] = layer(inps[j].unsqueeze(0).to(device)
                                , *model_args, **model_kwargs)[0].to("cpu")
        for h in handles:
            h.remove()

        for name in subset:
            print(f"pruning layer {i} name {name}")
            W_metric = torch.abs(subset[name].weight.data) * torch.sqrt(wrapped_layers[name].scaler_row.reshape((1,-1)))

            W_mask = (torch.zeros_like(W_metric) == 1)  ## initialize a mask to be all False
            if prune_n != 0:
                # structured n:m sparsity
                for ii in range(W_metric.shape[1]):
                    if ii % prune_m == 0:
                        tmp = W_metric[:,ii:(ii+prune_m)].float()
                        W_mask.scatter_(1,ii+torch.topk(tmp, prune_n,dim=1, largest=False)[1], True)
            else:
                sort_res = torch.sort(W_metric, dim=-1, stable=True)

                if args.use_variant:
                    # wanda variant 
                    tmp_metric = torch.cumsum(sort_res[0], dim=1)
                    sum_before = W_metric.sum(dim=1)

                    alpha = 0.4
                    alpha_hist = [0., 0.8]
                    W_mask, cur_sparsity = return_given_alpha(alpha, sort_res, W_metric, tmp_metric, sum_before)
                    while (torch.abs(cur_sparsity - args.sparsity_ratio)>0.001) and (alpha_hist[1]-alpha_hist[0]>=0.001):
                        if cur_sparsity > args.sparsity_ratio:
                            alpha_new = (alpha + alpha_hist[0]) / 2.0
                            alpha_hist[1] = alpha
                        else:
                            alpha_new = (alpha + alpha_hist[1]) / 2.0
                            alpha_hist[0] = alpha

                        alpha = alpha_new 
                        W_mask, cur_sparsity = return_given_alpha(alpha, sort_res, W_metric, tmp_metric, sum_before)
                    print(f"alpha found {alpha} sparsity {cur_sparsity:.6f}")
                else:
                    # unstructured pruning
                    indices = sort_res[1][:,:int(W_metric.shape[1]*args.sparsity_ratio)]
                    W_mask.scatter_(1, indices, True)

            subset[name].weight.data[W_mask] = 0  ## set weights to zero 

        for j in range(args.nsamples):
            with torch.no_grad():
                outs[j] = layer(inps[j].unsqueeze(0).to(device), *model_args, **model_kwargs)[0].to("cpu")
        inps, outs = outs, inps
        layer.to("cpu")
        clean()

    model.config.use_cache = use_cache 
    torch.cuda.empty_cache()


@torch.no_grad()
def prune_sparsegpt(args, model, tokenizer, dev, prune_n=0, prune_m=0,
                    percdamp=0.01, blocksize=128):
    ## SparseGPT code available at: https://github.com/IST-DASLab/sparsegpt/tree/f5c25005a61f96a0933ca2f95705a963585aafaa
    print('Starting ...')
    use_cache = model.config.use_cache 
    model.config.use_cache = False 
    dataloader, _ = get_loaders("c4",nsamples=args.nsamples,seed=args.seed,seqlen=model.seqlen,tokenizer=tokenizer)
    #adapt the dataloader to the format we are expecting
    tokens = torch.stack([d[0][0] for d in dataloader])

    inps, model_args, model_kwargs = get_inps(model, tokens, offload_activations=True, dev=dev)
    outs = torch.zeros_like(inps)

    layers = model.model.layers

    print('Ready.')
    
    #offload the entire model to cpu to save memory
    # model.to("cpu")

    for i in range(len(layers)):
        layer = layers[i]
        #cast layer to device
        layer.to(dev)
        

        subset = find_layers(layer)

        gpts = {}
        for name in subset:
            gpts[name] = SparseGPT(subset[name])

        def add_batch(name):
            def tmp(_, inp, out):
                gpts[name].add_batch(inp[0].data, out.data)
            return tmp

        handles = []
        for name in gpts:
            handles.append(subset[name].register_forward_hook(add_batch(name)))

        for j in range(args.nsamples):
            outs[j] = layer(inps[j].unsqueeze(0).to(dev), *model_args, **model_kwargs)[0].to("cpu")
        for h in handles:
            h.remove()

        for name in gpts:
            print(i, name)
            print('Pruning ...')

            gpts[name].fasterprune(args.sparsity_ratio, prune_n=prune_n, prune_m=prune_m, percdamp=percdamp, blocksize=blocksize)
            gpts[name].free()

        for j in range(args.nsamples):
            outs[j] = layer(inps[j].unsqueeze(0).to(dev), *model_args, **model_kwargs)[0].to("cpu")
        layer.to("cpu")
        clean()
        layers[i] = layer 
        torch.cuda.empty_cache()

        inps, outs = outs, inps

    model.config.use_cache = use_cache
    torch.cuda.empty_cache()



@torch.no_grad()
def prune_ablate(args, model, tokenizer, dev, prune_n=0, prune_m=0):
    ## SparseGPT code available at: https://github.com/IST-DASLab/sparsegpt/tree/f5c25005a61f96a0933ca2f95705a963585aafaa
    print('Starting ...')
    dataloader, _ = get_loaders("c4",nsamples=args.nsamples,seed=args.seed,seqlen=model.seqlen,tokenizer=tokenizer)

    use_cache = model.config.use_cache
    model.config.use_cache = False
    layers = model.model.layers

    if "model.embed_tokens" in model.hf_device_map:
        dev = model.hf_device_map["model.embed_tokens"]

    dtype = next(iter(model.parameters())).dtype
    inps = torch.zeros(
        (args.nsamples, model.seqlen, model.config.hidden_size), dtype=dtype, device=dev
    )
    cache = {'i': 0, 'attention_mask': None, "position_ids": None}

    class Catcher(nn.Module):
        def __init__(self, module):
            super().__init__()
            self.module = module
        def forward(self, inp, **kwargs):
            inps[cache['i']] = inp
            cache['i'] += 1
            cache['attention_mask'] = kwargs['attention_mask']
            cache['position_ids'] = kwargs['position_ids']
            raise ValueError
    layers[0] = Catcher(layers[0])
    for batch in dataloader:
        try:
            model(batch[0].to(dev))
        except ValueError:
            pass
    layers[0] = layers[0].module
    torch.cuda.empty_cache()

    outs = torch.zeros_like(inps)
    attention_mask = cache['attention_mask']
    position_ids = cache['position_ids']

    print('Ready.')

    for i in range(len(layers)):
        layer = layers[i]
        if f"model.layers.{i}" in model.hf_device_map:
            dev = model.hf_device_map[f"model.layers.{i}"]
            print(f"layer {i} device {dev}")
            inps, outs, attention_mask, position_ids = inps.to(dev), outs.to(dev), attention_mask.to(dev), position_ids.to(dev)

        subset = find_layers(layer)

        gpts = {}
        for name in subset:
            gpts[name] = AblateGPT(subset[name])

        def add_batch(name):
            def tmp(_, inp, out):
                gpts[name].add_batch(inp[0].data, out.data)
            return tmp

        handles = []
        for name in gpts:
            handles.append(subset[name].register_forward_hook(add_batch(name)))

        for j in range(args.nsamples):
            outs[j] = layer(inps[j].unsqueeze(0), attention_mask=attention_mask, position_ids=position_ids)[0]
        for h in handles:
            h.remove()

        for name in gpts:
            print(i, name)
            print('Pruning ...')

            if args.prune_method == "ablate_wanda_seq":
                prune_mask = gpts[name].get_wanda_mask(args.sparsity_ratio, prune_n, prune_m)
            elif args.prune_method == "ablate_mag_seq":
                prune_mask = gpts[name].get_mag_mask(args.sparsity_ratio, prune_n, prune_m)
            elif "iter" in args.prune_method:
                prune_mask = None 

            gpts[name].fasterprune(args, args.sparsity_ratio, mask=prune_mask, prune_n=prune_n, prune_m=prune_m, percdamp=0.01, blocksize=128)
            gpts[name].free()

        for j in range(args.nsamples):
            outs[j] = layer(inps[j].unsqueeze(0), attention_mask=attention_mask, position_ids=position_ids)[0]

        layers[i] = layer 
        torch.cuda.empty_cache()

        inps, outs = outs, inps

    model.config.use_cache = use_cache
    torch.cuda.empty_cache()